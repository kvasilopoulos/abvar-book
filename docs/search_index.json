[
["intro.html", "Chapter 1 Introduction 1.1 Features and namimg convention 1.2 Estimation 1.3 Prior tools for BVARs 1.4 Identification 1.5 Applications/Methods 1.6 Extra 1.7 Tests 1.8 Counterfactuals 1.9 Forecast Scenarios 1.10 Diagnostics 1.11 See also other packages", " Chapter 1 Introduction 1.1 Features and namimg convention fa: Factor Augementing sv: Stochastic Volatility tvp: Time Varying Parametrs 1.2 Estimation 1.2.1 Classical Models var_ls : Classical var_ml : Maximum Likelihood (for illustration purposes) var_gls: Generalized Linear Squares var_bc : Bias-Correction var_tvp: Time Varing Parameter var_sv: Stochastic Volatility var_tvp_sv: Time Varing Parameter &amp; Stochastic Volatility favar: Factor Augmented favar_tvp: Factor Augement Time Varying Parameter bvar_lrg: Large Bayesian VAR (see pkg lbvar) 1.2.2 Markov Switching msvar: regime-switching models with fixed transition probabilities 1.2.3 Bayesian VAR with analytical Solution bvar_diff: Diffuse prior bvar_conj: Conjucate prior bvar_minn: Minnesota prior bvar_ss : Steady State prior 1.2.4 Bayesian VAR with Gibbs Sampler bvar_svss: Stochastic Search Variables Selection bvar_fvs : Flexible Variable Selection bvar_cnw : Conditional Normal-inverse-Wishart prior bvar_inw : Independent Normal-inverse-Wishart prior 1.3 Prior tools for BVARs Hyperparameter optimasation by grid search Block exogeneity Dummy observation … 1.4 Identification id_chol: Choleski factorisation (short-run restrictions) id_triang:Triangluar factorisation (short-run restrictions) id_lr: Long run restrictions id_sign: Sign, magnitude and zero restrictions (Arias et al., 2014) id_iv: Instrumental variables estimation Stock and Watson (2012) &amp; Mertens and Ravn (2013) 1.5 Applications/Methods irf: Impulse response functions forecast: Uncondtional forecasts fevd:Forecast error variance decomposition hd: Historical Decomposition forecast_shock: Condtional forecasts: shock approach (Waggoner and Zha, 1999) forecast_tilt: Condtional forecasts: tilting approach (Robertson et al., 2005) forecast_eval: Forecast Evaluation 1.6 Extra Non-fundamentanless Lag Order Selection Procedures Top-down Sequential Testing Bottom-up Sequential Testing Granger Causality vecm Information Criteria (aic, hqc, sic) 1.7 Tests Residual Autocorrelation Portmanteau Test for Residual Autocorrelation LM Test for Residual Autocorrelation Stability Time Invariance Heteroskedasticity Normality 1.8 Counterfactuals Simulation Policy 1.9 Forecast Scenarios 1.10 Diagnostics Convergence test Histograms of the posterior Gaussian kernel estimation Recursive Moments Trace plots 1.10.1 Accompanying Filtering (forthcoming transx) Simulation (forthcoming simdgp) Data (forthcoming econdata) Unit Root test (see urca) 1.11 See also other packages vars svars bvars: https://github.com/joergrieger/bvars bvarsv:https://cran.r-project.org/web/packages/bvarsv/index.html lbvar: https://github.com/gabrielrvsc/lbvar bvartools: https://github.com/franzmohr/bvartools bvar: https://github.com/nk027/bvar bvarrKK: https://github.com/bdemeshev/bvarrKK mfbvar: https://github.com/ankargren/mfbvar BMR GediminasB/bayesVAR_TVP ragt2ridges "],
["estimation.html", "Chapter 2 Estimation 2.1 Reduce form VAR(p) 2.2 Structural Form VAR(p) 2.3 Compact Form 2.4 Minessota prior", " Chapter 2 Estimation 2.1 Reduce form VAR(p) \\[y_{t}= A_1y_{t-1} + \\cdots + A_p y_{t-p} + u_t\\] 2.2 Structural Form VAR(p) \\[B_0y_{t}= B_{1}y_{{t-1}}+\\cdots +B_{p}y_{{t-p}}+e_{t}\\] Let try some text here: \\(E(e_t) = 0\\) — every error term has mean zero \\(\\alpha\\) \\(E (e_{t}e_{t}&#39;)= \\Omega\\) — the contemporaneous covariance matrix of error terms is \\(\\Omega\\) (a k × k positive-semidefinite matrix) \\(E(e_{t}e_{{t-k}}&#39;)=0\\) — for any non-zero k — there is no correlation across time; in particular, no serial correlation in individual error terms.[1] \\[y_{t}= \\underbrace{B_0^{-1}B_{1}y_{{t-1}}}_{A_1} + \\cdots + \\underbrace{B_0^{-1}B_{p}y_{{t-p}}}_{A_p} + \\underbrace{B_0^{-1}e_{t}}_{w_t}\\] 2.3 Compact Form Consider the VAR(p) model (??) written in more compact form $$y_t = \\[Y = AZ + U\\] where \\(Z = [1, y_{t-1}&#39;, \\cdots, y_{t-p}&#39;]\\) \\[\\hat{A} = [\\hat{\\nu_t}, \\hat{A_1}, \\cdots, \\hat{A_p}] = (Z&#39;Z)^{-1}Z&#39;Y\\] \\[\\hat{U} = Y - Z\\hat{A}\\] \\[\\hat{\\Sigma_u} = \\dfrac{(\\hat{U}&#39; \\hat{U})^{-1}}{(N-Kp-p-1)}\\] 2.3.1 Impulse Response Function Given \\(B_)\\) and \\(u_t\\), we immediately obtain \\(w_t = B_0u_t\\). Having identified the structural shocks \\(w_t\\), our interest usually is not in the shocks themselves, how- ever, but in the responses of each element of yt = (y1t,…,yKt) to a one-time impulse in wt = (w1t,…,wKt) \\[\\dfrac{∂yt+i}{∂w_t} = \\Theta_i, \\quad i = 0, 1, 2, . . . , H, \\] where Θi is a K ×K matrix. The elements of this matrix for given i are denoted as \\[ θ_{jk,i} = \\dfrac{∂y_{j,t+i}}{∂w_{kt}} \\] such that \\(\\Theta_i = [\\theta_{jk,t}].\\) By successive substitution for \\(Y_{t−i}\\), equation can be written as \\[ Y_{t+i} = A_{i+1}Y_{t−1} + \\sum^i_{j=0} A_jU_{t+i−j}\\] Left-multiplying this equation by \\(J ≡ [I_K, 0_{K×K(p−1)}]\\) yields \\[\\begin{align} y_{t+i} = &amp; JA_{i+1}Y_{t−1} + \\sum^{i}_{j=0} JA^jU_{t+i−j} \\\\ = &amp; JA^{i+1}Y_{t−1} + \\sum^i_{j=0} JA^{j}J&#39;JU_{t+i−j} \\\\ = &amp; JA^{i+1}Y_{t−1} + \\sum^i_{j=0} JA^{j}Ju_{t+i−j} \\\\ \\end{align}\\] Thus, the response of the variable j = 1,…,K in the VAR(p) system to a unit shock ukt, k = 1,…,K, i periods ago, is given by: \\[ \\underset{K×K}{\\Phi_i} = [\\phi_{jk,i}] ≡ JA^iJ&#39;\\] The \\(\\phi_i\\) are also sometimes referred to as responses to VAR forecast errors, as dynamic multipliers, or simply as reduced-form impulse responses. 2.4 Minessota prior The simplest form of prior distributions for VAR models is known as the Minnesota (or Litterman) prior. In this framework, it is assumed that the VAR residual variance-covariance matrix \\(\\Sigma\\) is known. Hence, the only object left to estimate is the vector of parameters \\(\\beta\\). To obtain the posterior distribution for \\(\\beta\\) from 1.2.3, one needs two elements: the likelihood function \\(f(y |\\beta)\\) for the data, and a prior distribution \\(\\pi(\\beta)\\) for \\(\\beta\\). \\(y\\sim \\mathcal{N}(\\tilde{X}\\beta, \\tilde{\\Sigma})\\) Therefore, one may write the likelihood for y as: \\[f(y|\\beta, \\tilde{\\Sigma}) = (2\\pi)^{-nT/2} |\\tilde{\\Sigma}|^{-1/2}\\exp[-\\dfrac{1}{2}(y-\\tilde{X}\\beta)\\tilde{\\Sigma}^{-1}(y-\\tilde{X}\\beta)]\\] Ignoring terms independent from \\(\\beta\\) relegated to proportionality simplifies to: \\[f(y|\\beta, \\tilde{\\Sigma})\\propto \\exp[-\\dfrac{1}{2}(y-\\tilde{X}\\beta)\\tilde{\\Sigma}^{-1}(y-\\tilde{X}\\beta)]\\] Now turn to the prior distribution for \\(\\beta\\). It is assumed that \\(\\beta\\) follows a multivariate normal distribution, with mean \\(\\beta_0\\) and covariance matrix \\(\\Omega_0\\): \\[\\pi(\\beta) \\sim \\mathcal{N}(\\beta_0, \\Omega_0)\\] To identify \\(\\beta_0\\) and \\(\\Omega_0\\), Litterman (1986) proposed the following strategy. As most observed macroeconomic variables seem to be characterized by a unit root (in the sense that their changes are impossible to forecast), our prior belief should be that each endogenous variable included in the model presents a unit root in its first own lags, and coefficients equal to zero for further lags and cross-variable lag coefficients. In the absence of prior belief about exogenous variables, the most reasonable strategy is to assume that they are neutral with respect to the endogenous variables, and hence that their coefficients are equal to zero as well. These elements translate into \\(\\beta_0\\) being a vector of zeros, save for the entries concerning the first own lag of each endogenous variable which are attributed values of 1. Note though that in the case of variables known to be stationary, this unit root hypothesis may not be suitable, so that a value around 0.8 may be preferred to a value of 1. "],
["methods.html", "Chapter 3 Methods", " Chapter 3 Methods We describe our methods in this chapter. "],
["applications.html", "Chapter 4 Applications 4.1 Example one 4.2 Example two", " Chapter 4 Applications Some significant applications are demonstrated in this chapter. 4.1 Example one 4.2 Example two "],
["final-words.html", "Chapter 5 Final Words", " Chapter 5 Final Words We have finished a nice book. "],
["references.html", "References", " References "],
["literature.html", "Chapter 6 Literature", " Chapter 6 Literature Here is a review of existing methods. You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 1. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter 3. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 6.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 6.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 6.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 6.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package [@R-bookdown] in this sample book, which was built on top of R Markdown and knitr [@xie2015]. "],
["favar.html", "Chapter 7 FAVAR", " Chapter 7 FAVAR Factor augmented VAR model introduce in Bernanke et (2005). The FAVAR model can be writtern compactly as \\[y_{i,t} = \\lambda_i f_t = \\gamma_i r_t + \\epsilon_{it}\\] "],
["algo.html", "Chapter 8 Algorithms 8.1 IRF 8.2 Error Bands for Impulse Response Function", " Chapter 8 Algorithms 8.1 IRF 8.1.1 Companion 8.1.2 Algorithm 1 Create a 2D-array of \\(Kp \\times (h+1) \\times Kp\\) Assign the shock to the first observation (unit, se, etc) Iterate over the horizons 1, 2, …, h where \\[ irf_{i+1} = irf_{i} A&#39;\\] 8.1.3 Algorithm 2 Create a array \\(J = [I_k, 0_{K\\times K(p-1)}]\\) Create an 3D-array of \\(Kp \\times (h + 1) \\times Kp\\) to store the irfs Assign the shock to the first observation (unit, se, etc) Iterate over the horizons for i = 1,2, …, h where \\[ irf_{i+1} = J A^i J&#39;\\] 8.2 Error Bands for Impulse Response Function 8.2.1 Asymptotic 8.2.2 Mote Carlo 8.2.3 Bootstrap 8.2.4 Bootstrap after Bootstrap Montecarlo method proceeds as follows.1. DrawˆπlfromN(ˆπ,ˆΩ⊗ˆQ−1).2. computeC(L)l.3. Repeat 1-2M(withMbig, i.e.1000) times.4. For all the elementsCi,j,h,i,j= 1,…,n,h= 1,2,…of the impulse responsefunctions collect theαth and 1−αth percentile across the`draws as aconfidence interval forCi,j,h. 8.2.5 Algorithm 2.2.1 (impulse response functions, all priors): Define the number of iterations (It − Bu) of the algorithm, and the time horizon h. Fix i = 1. Then set yi,T = 1. At iteration n, draw \\(\\beta_{n}\\) from its posterior distributions. Simply recycle draw n from the Gibbs sampler. Generate recursively the simulated values \\(\\tilde{y}^{(n)}_{T+1}\\), \\(\\tilde{y}^{(n)}_{T+2}\\), \\(\\ldots\\), \\(\\tilde{y}^{(n)}_{T+h}\\) from 1.1.2: \\(\\tilde{y}^{(n)}_{T+1} = A_1y_t + A_2y_{t-1} + \\ldots + A_py_{T+1-p}\\). Once ˜yT+1 is computed, use: ˜y T+2 = A1˜y T+1 + A2yT + … + ApyT+2−p And continue this way until ˜y T+h is obtained. Once again, both the exogenous terms and the shocks are ignored since they are assumed to take a value of 0 at all periods. The values of A, 1,A, 2…A, p come from \\(\\beta_{n}\\). Discard \\(\\beta_{n}\\) to obtain draws ˜y T+1, ˜y T+2…˜y T+h from the predictive distribution f(yT+1:T+h |yT ). Repeat until (It−Bu) iterations have been performed. This produces: n ˜y T+1 |yT , ˜y T+2 |yT , …, ˜y T+h |yT oIt−j=a sample of independent draws from the joint predictive distribution in the case yi,T = 1. Go back to step 2, and fix i = 2. Then go through steps 3-6 all over again. Then repeat the process for i = 3, …, n. This generates the impulse response functions for all the shocks in the model. "]
]
